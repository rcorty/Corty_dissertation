\chapter{The Heteroscedastic Linear Mixed Model}
\label{chap:het_LMM}

\cref{chap:bvh} described a statistical approach that stands on the assumption that all individuals in the mapping population are equally related.
This assumption is appropriate in the analysis of experimental crosses because it is nearly true.
It has been shown that a more general approach that accounts for the slight differential relatedness in an F2 intercross delivers little or no additional benefit in terms of mapping power or precision \citep{Parker2014b}.
\cref{chap:dglm_human} described a statistical approach to QTL mapping that accounts for differential relatedness in the mapping population by including principle components of the genetic variation as regressors.
This approach is heuristic, correcting for large-scale relationships, but not finer grained ones.
% Previous chapters dealt with differential relatedness obliquely, either assuming its absence or using a heuristic to correct for it, this chapter addresses it directly.
This chapter describes the linear mixed model (LMM), which can accommodate any pattern of differential relatedness a mapping population may have, at both large and fine scales.

The LMM models an observed phenotype, $\by$, as,
\begin{align}
	\by	&= \mathbf{1}\mu + \bX\bbeta + \bG\balpha + \ba + \be  \label{eq:lmm}
\end{align}

where $\mathbf{1}$ is a column vector of ones, $\bX$ is the design matrix of covariates, $\bG$ is the design matrix of focal genetic effects, and $\mu$, $\bbeta$, and $\balpha$ are unconstrained parameters that can be referred to as the population mean, the effect(s) of the covariate(s), and the effect(s) of the focal genetic factor(s), respectively.

$\ba$ and $\be$ are so-called ``random effects'', estimated in the process of model fitting, but with constraints.
Specifically, they are modeled hierarchically as
\begin{align}
    \ba &\sim \N(0, \bK \tau^2) ,   \label{eq:a}\\
    \be &\sim \N(0, \bD \sigma^2)   \label{eq:e}
\end{align}
where $\bK$ is a known, positive semi-definite genomic similarity matrix, and $\bD$ is a known diagonal residual variance matrix.
The scale parameters, $\tau^2$ and $\sigma^2$, are constrained only to be non-negative.

In the context of genetic mapping, the this model is fit to each putative QTL, using $\bG$ to encode the locus design matrix and testing whether $\balpha = \bm{0}$.
If $\balpha \neq \bm{0}$, the locus is a QTL.

Two statistical tests used to determine whether $\balpha = \bm{0}$, the $t$ test and the likelihood ratio test (LRT), require computation of the values of the parameters that maximize the likelihood of the data.
\citet{Henderson1984} described a suite of procedures for computing these maximum likelihood parameter values in a variety of situations.

However, Henderson's methods are of limited use in QTL mapping.
His focus was on estimation of breeding values ($\ba$ in \cref{eq:lmm}) and therefore the model only needed to be fit once, to one design matrix, and therefore speed was not a primary concern.
Therefore, Henderson's methods are not of great use in the context of QTL mapping, where each putative QTL demands its own maximum likelihood model fit.




% many different values of $\bX$... and compare to one with no genetic in $\bX$ to do LRT.
% The default way to do so would be to use Henderson's method each time, but this is very slow.
% A recent development described a matrix math trick to make this process fast when $\bD = \bI$.
% I first summarize that trick and then go on to describe a further trick that allows for fast fitting for any diagonal $\bD$.


% [define animal model, y, K, Z, D, two variance components]
% [all diagonal elements of K are 1, off diags can be also]

\newpage
\section{A simpler, but equivalent, parametrization}

The LMM as specified in \cref{eq:lmm} is equivalent to:
\begin{align}
    \by &\sim \N(\bX_c\bbeta_c, \bSigma)
\end{align}
where fixed effects design matrices are collapsed into $\bX$ and the variance-covariance matrices of the random effects are combined into $\bSigma$.
Specifically,
\begin{align}
    \bX_c     &= \left[1 \enspace \bX \enspace \bG \right]\\
    \bbeta_c  &= \left[\mu \enspace \bbeta\T \balpha\T \right]\T\\
    \bSigma &= \bK \tau^2 + \bD \sigma^2
\end{align}
Going forward, we refer to $\bX_c$ and $\bbeta_c$ as simply $\bX$ and $\bbeta$ for simplicity.

This model can be re-parametrized to directly use the well-known narrow sense heritability, increasing its interpretability.
Specifically, $h^2 = \frac{\tau^2}{\tau^2 + \sigma^2}$ and $\lambda = \tau^2 + \sigma^2$.
\begin{align}
  \bSigma &= \left(\bK \frac{\tau^2}{\tau^2 + \sigma^2} + \bD \frac{\sigma^2}{\tau^2 + \sigma^2}\right) (\tau^2 + \sigma^2)\\
      &= \bK h^2 + \bD (1 - h^2) \lambda
\end{align}


\section{Given \texorpdfstring{$h^2$}{h-squared}, the LMM Reduces to Multiple Linear Regression}

[transition]
Thus, for any given value of $h^2$, we can define $\bV = \bK h^2 + \bD (1 - h^2)$, and the LMM reduces to a standard linear model (SLM):
\begin{align}
    \by \sim \N(\bX\bbeta, \bV \lambda)     \label{eq:gls}
\end{align}

with log likelihood:
\begin{align}
    \ell(\bbeta, \lambda; \by, \bX, \bV) &= 
        -\frac{n}{2}\log(2\pi)
        -\frac{1}{2}\log|\bV\lambda|
        -           \frac{1}{2\lambda}
            (\by-\bX\bbeta)\T\bV\inv(\by-\bX\bbeta) \\
    &= 
        -\frac{n}{2}\log(2\pi)
        -\frac{1}{2}\log|\bV|
        -\frac{n}{2}\log\lambda
        - \frac{1}{2\lambda}
            (\by-\bX\bbeta)\T\bV\inv(\by-\bX\bbeta)
\end{align}

And there is a known, closed-form, maximum likelihood estimator for its two parameters:
\begin{align}
	\widehat{\bbeta}    &= (\bX \T \bV \inv \bX) \inv \bX \T \bV \inv \by\\
    \widehat{\lambda}   &= \norm{(\bX\widehat{\bbeta} - \by)\T(\bX\widehat{\bbeta} - \by)}{2}
\end{align}

In the context of a genome scan, we want to calculate these maximum likelihood estimators for a single phenotype, $\by$, genomic similarity, $\bK$ residual variance, $\bD$, and many different values of $\bX$ --- recall that the genetic variant(s) being investigated are in $\bG$.
Also recall that, though we have described a parameterization that recapitulates a standard linear regression problem when $h^2$ is known, we do not, in general know the true value of $h^2$, so we will additionally want to optimize over all possible values of $h^2 \in [0, 1]$.

We now describe a matrix algebra trick that allows us to increases the up-front computational cost of fitting the linear regression model \cref{eq:gls}, but drastically decreases the computational cost for each new value of $\bX$.


\section{Rotation to Independence}

Imagine we knew of a multiplier matrix, $\bM$ such that
\begin{align}
	\bM \T \bM = \bV \inv
\end{align}
 
Let us now consider a new, rotated phenotype vector, $\by_r = \bM \by$, a new covariate matrix, $\bX_r = \bM \bX$, and a new SLM.
\begin{align}
    \by_r &\sim \N(\bX_r \bbeta_r, \, \bI \lambda_r)
\end{align}

The log likelihood of this SLM is:
\begin{align}
    \ell(\bbeta_r, \lambda_r; \bX_r, \by_r) 
    &=    -\frac{n}{2}\log(2\pi)
          -\frac{1}{2}\log|\bI|
          -\frac{n}{2}\log\lambda_r
          -\frac{1}{2\lambda_r}
          (\by_r - \bX_r)\T (\by_r - \bX_r\bbeta_r)\\
    & =
          -\frac{n}{2}\log(2\pi)
          -\frac{n}{2}\log\lambda_r
          -\frac{1}{2\lambda_r}
          (\bM\by - \bM\bX\bbeta)\T (\bM\by - \bM\bX\bbeta)\\
    &=    -\frac{n}{2}\log(2\pi)
          -\frac{n}{2}\log\lambda_r
          -\frac{1}{2\lambda_r}
          (\by - \bX\bbeta)\T \bM\T \bM (\by - \bX\bbeta)\\
    &=    -\frac{n}{2}\log(2\pi)
          -\frac{n}{2}\log\lambda_r
          -\frac{1}{2\lambda_r}
          (\by-\bX\bbeta)\T \bV\inv (\by-\bX\bbeta)
\end{align}

and its ML estimators are:
\begin{align}
    \widehat{\bbeta_r}    &= (\bX_r \T \bX_r) \inv \bX_r \T \by_r\\
    \widehat{\lambda_r}   &= \norm{(\bX_r \widehat{\bbeta_r} - \by_r)\T(\bX_r \widehat{\bbeta_r} - \by_r)}{2}
\end{align}

% Recalling that We are now considering a model of the form:
% \begin{align}
%     \mathbf{My} &\sim \Norm(\mathbf{MX}\boldsymbol{\beta}_z, \, \mathbf{I}\sigma^2_\text{z})
% \end{align}


% The expectation and variance of $\by_m$ are:
% \begin{align}
% \E(\by_m)	&= \E(\bM \by) \\
% 			&= \bM \E(\by)\\
% \V(\by_m)	&= \V(\bM \by)\\
% 			&= \bM \V(\by) \bM\T
% \end{align}


% Note the similarity of their log likelihoods:

Thus we have established that, given $\bM$ we can fit the model


\section{Rotation for the Homoscedastic Model}

[D = I]
Kang proposed the following [cite EMMA].




\section{Rotation for the Heteroscedastic Model}

The above $\bM$ relied on [point out where D=I is required] [in step X].
Generally, however, some phenotypes are known with more certainty than others.
Want to have a diagonal D, but not necessarily I.

\subsection{Proposal}

\begin{align}
	\bM &= (\bLL + \delta \bI)^{-\frac{1}{2}}\bUL\T \bD^{-\frac{1}{2}}\\
\intertext{where}
	\bL &= \bD^{-\frac{1}{2}}\bK \bD^{-\frac{1}{2}}\\
\intertext{and}
	\bL &= \bUL \bLL \bUL^T
\end{align}
is its eigen decomposition


\subsection{Validity}

To be a valid multiplier matrix, $\bM$ must have the property:
\begin{equation}
  \bM \T \bM = \bV\inv
\end{equation}
	


% I verify this by showing that
% \[
% 	\bV \bM\T \bM = \bM\T \bM \bV = \bI
% \]

% I verify by direct calculation:



\begin{proof}
First, derive a useful form of $\bV$.
\begin{align}
\bV &= \bK + \delta \bD                                                                                             \tageq{definition}\\
    &= \bD\halfpow \bD\neghalfpow (\bK + \delta \bD)                                                                \tageq{pre-multiply by $\bD\halfpow \bD\neghalfpow = \bI$}\\
    &= \bD\halfpow \bD\neghalfpow (\bK + \delta \bD) \bD\neghalfpow \bD\halfpow 										                \tageq{post-multiply by $\bD\neghalfpow \bD\halfpow = \bI$}\\
    &= \bD\halfpow(\bD\neghalfpow \bK \bD\neghalfpow + \delta \bD\neghalfpow \bD \bD\neghalfpow)\bD\halfpow         \tageq{distribute $\bD\neghalfpow$ in}\\
    &= \bD\halfpow(\bD\neghalfpow \bK \bD\neghalfpow + \delta \bI) \bD\halfpow                                  		\tageq{definition of root inverse}\\
    &= \bD\halfpow(\bL + \delta \bI) \bD\halfpow                                                                 		\tageq{define: $\bL = \bD\neghalfpow\bK \bD\neghalfpow$}\\
    &= \bD\halfpow(\bUL \bLL \bUL \T + \delta \bI) \bD\halfpow                                                			\tageq{eigen decomposition of $\bL$}\\
    &= \bD\halfpow(\bUL \bLL \bUL \T + \delta \bUL \bUL \T)\bD\halfpow                                         			\tageq{property of eigen vectors}\\
    &= \bD\halfpow\bUL (\bLL + \delta \bI) \bUL \T \bD\halfpow                                                			\tageq{distributive property}
\end{align}
and invert it
\begin{align}
\bV\inv &= \left( \bD\halfpow\bUL (\bLL + \delta \bI) \bUL \T \bD\halfpow \right)\inv                                         \tageq{definition}\\
        &= \left( \bD\halfpow \right)\inv \left( \bUL (\bLL + \delta \bI) \bUL\T \right)\inv \left( \bD\halfpow \right)\inv   \tageq{inverse of product}\\
        &= \bD\neghalfpow \left( \bUL (\bLL + \delta \bI) \bUL\T \right)\inv \bD\neghalfpow                                   \tageq{inverse of diagonal matrix}\\
        &= \bD\neghalfpow \bUL (\bLL + \delta \bI)\inv \bUL\T  \bD\neghalfpow                                                 \tageq{inverse of eigen decomposition}
\end{align}
and compare to $\bM\T \bM$
\begin{align}
\bM \T \bM 	&= \left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow 	\right)\T   \left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow \right) \tageq{definition}\\
			&= \left(\bD\neghalfpow \bUL (\bLL + \delta \bI)\neghalfpow 	\right)  \left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow		\right)          \tageq{transpose of product}\\
			&= \bD\neghalfpow \bUL \left( (\bLL + \delta \bI)\neghalfpow 	(\bLL + \delta \bI)\neghalfpow \right)	\bUL\T \bD\neghalfpow                          \tageq{associative property}\\
			&= \bD\neghalfpow \bUL (\bLL + \delta \bI)\inv \bUL\T \bD\neghalfpow                                                                                 \tageq{definition of root inverse}
			% &= \bD\neghalfpow \left( \bUL (\bLL + \delta \bI) \bUL\T \right)\inv \bD\neghalfpow 																\tag*{inverse of eigen decomposition}\\
\end{align}
\end{proof}


\subsection{Calculation of Necessary Quantities}

\subsubsection{M itself}


\subsubsection{something else}


\subsubsection{Its determinant}

% Now we have
% \begin{align}
% \bM\T \bM 	&= 	\left( \bD\halfpow\bUL (\bLL + \delta \bI) \bUL \T \bD\halfpow 		\right)		
% 					\left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow 	\right)\T
% 					\left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow 	\right) 				\tag*{definitions}\\
% 				&= 	\left( \bD\halfpow\bUL (\bLL + \delta \bI) \bUL\T \bD\halfpow 		\right)
% 					\left(\bD\neghalfpow \bUL (\bLL + \delta \bI)\neghalfpow 	\right)
% 					\left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow 	\right)					\tag*{transpose of product}\\
% 				&= 	\bD\halfpow\bUL (\bLL + \delta \bI) \bUL\T \left(\bD\halfpow 		
% 					\bD\neghalfpow \right) \bUL \left((\bLL + \delta \bI)\neghalfpow
% 					(\bLL + \delta \bI)\neghalfpow \right) \bUL\T \bD\neghalfpow						\tag*{associative property of matrix multiplication}\\
% 				&= 	\bD\halfpow\bUL (\bLL + \delta \bI) \bUL\T
% 					 \bUL \left(\bLL + \delta \bI \right)\inv  \bUL\T \bD\neghalfpow					\tag*{definition of root and root inverse}\\
% 				&= 	\bD\halfpow\bUL (\bLL + \delta \bI) \left(\bLL + \delta \bI \right)\inv  \bUL\T \bD\neghalfpow					\tag*{property of eigenvector matrix}\\
% 				&= 	\bD\halfpow\bUL  \bUL\T \bD\neghalfpow					\tag*{definition of inverse}\\
% 				&= 	\bD\halfpow \bD\neghalfpow					\tag*{property of eigenvector matrix}\\
% 				&= \bI 													\tag*{definition of inverse}
% \end{align}

% 	&= [D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) U_L^T D^{\frac{1}{2}}] [D^{-\frac{1}{2}}U_L(\Lambda_L+\delta I)^{-\frac{1}{2}}][(D^{-\frac{1}{2}}U_L(\Lambda_L+\delta I)^{-\frac{1}{2}})^T]\\
%     		&= D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) U_L^T D^{\frac{1}{2}}D^{-\frac{1}{2}}U_L(\Lambda_L+\delta I)^{-\frac{1}{2}}(\Lambda_L+\delta I)^{-\frac{1}{2}T} U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) U_L^T D^{\frac{1}{2}}D^{-\frac{1}{2}}U_L(\Lambda_L+\delta I)^{-1} U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) U_L^T U_L(\Lambda_L+\delta I)^{-1} U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) (\Lambda_L+\delta I)^{-1} U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}}U_L U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}} D^{-\frac{1}{2}T} \\
%     		&= I
% \end{align}


\section{Simulation Results}

\section{Software}