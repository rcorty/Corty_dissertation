\chapter{The Heteroscedastic Linear Mixed Model}

Whereas the previous chapters have dealt with differential relatedness obliquely, either assuming its absence or using a heuristic to correct for it, this chapter addresses it directly.
The linear mixed model (LMM) [posulates] an observed phenotype, $\by$, as the sum of five unobserved, but estimated quantities: the population mean, $\mu$, the fixed effect deviation, $\bX\bbeta$, effect of the focal genetic variant(s), $\bG\balpha$, the genomic value, $\ba$, and the residual deviation, $\be$.
\begin{align}
	\by	&= \mathbf{1}\mu + \bX\bbeta + \bG\balpha + \ba + \be  \label{eq:lmm}
\end{align}
where $\mathbf{1}$ is a column vector of ones, $\bX$ is a matrix of covariates, $\bG$ is a design matrix of focal genetic effects, and $\mu$, $\bbeta$, and $\balpha$ are unconstrained, estimated quantities.

$\ba$ and $\be$ are so-called ``random effects'', estimated in the process of model fitting, but with [constraints].
Specifically, they are modeled hierarchiccally as
\begin{align}
    \ba &\sim \N(0, \bK \tau^2) ,   \label{eq:a}\\
    \be &\sim \N(0, \bD \sigma^2)   \label{eq:e}
\end{align}
where $\bK$ is a known, positive semi-definite genomic similarity matrix, and $\bD$ is a known diagonal residual variance matrix.
The scale parameters, $\tau^2$ and $\sigma^2$, are constrained only in the sense that they must be non-negative.

In the context of genetic mapping, the this model is used to test whether $\balpha = \bm{0}$.
If $\balpha \neq \bm{0}$, the genetic factor(s) encoded in $\bG$ influences the phenotype.

The two tests used to test whether $\balpha = \bm{0}$, the $t$ test and the likelihood ratio test (LRT), require [computation] of the values of the parameters that maximize the likelihood of the data.
A suite of procedures for computing these maximum likelihood parameter values in a variety of situations was described by Henderson [cite].
The main interest of Henderson, and of the animal breeding community in general, is the ``genetic value'', $\ba$ in the model.
And it's typically fit in the absence of $\bG$, or for very few different values of $\bG$ for marker assissted selection.



% many different values of $\bX$... and compare to one with no genetic in $\bX$ to do LRT.
% The default way to do so would be to use Henderson's method each time, but this is very slow.
% A recent development described a matrix math trick to make this process fast when $\bD = \bI$.
% I first summarize that trick and then go on to describe a further trick that allows for fast fitting for any diagonal $\bD$.


% [define animal model, y, K, Z, D, two variance components]
% [all diagonal elements of K are 1, off diags can be also]

\section{Given $h^2$, the LMM Reduces to Mutiple Linear Regression}

The LMM is equivalent to:
\begin{align}
    \by &\sim \N(\bX_c\bbeta_c, \bSigma)
\end{align}
where fixed effects design matrices are collapsed into $\bX$ and the variance-covariance matrices of the random effects are combined into $\bSigma$.
Specifically,
\begin{align}
    \bX_c     &= \left[1 \enspace \bX \enspace \bG \right]\\
    \bbeta_c  &= \left[\mu \enspace \bbeta\T \enspace \balpha\T \right]\T\\
    \bSigma &= \bK \tau^2 + \bD \sigma^2
\end{align}
Going forward, we refer to $\bX_c$ and $\bbeta_c$ as simply $\bX$ and $\bbeta$ for simplicity.

This model can be reparametrized to directly use the well-known narrow sense heritability, increasing its interpretability.
Specifically, $h^2 = \frac{\tau^2}{\tau^2 + \sigma^2}$ and $\lambda = \tau^2 + \sigma^2$.
\begin{align}
	\bSigma &= \left(\bK \frac{\tau^2}{\tau^2 + \sigma^2} + \bD \frac{\sigma^2}{\tau^2 + \sigma^2}\right) (\tau^2 + \sigma^2)\\
			&= \bK h^2 + \bD (1 - h^2) \lambda
\end{align}
Thus, for any given value of $h^2$, we can define $\bV = \bK h^2 + \bD (1 - h^2)$, and the LMM reduces to a standard linear model (SLM):
\begin{align}
    \by \sim \N(\bX\bbeta, \bV \lambda)     \label{eq:gls}
\end{align}

This SLM has log likelihood:
\begin{align}
    \ell(\bbeta, \lambda; \by, \bX, \bV) &= 
        -\frac{n}{2}\log(2\pi)
        -\frac{1}{2}\log|\bV\lambda|
        -           \frac{1}{2\lambda}
            (\by-\bX\bbeta)\T\bV\inv(\by-\bX\bbeta) \\
    &= 
        -\frac{n}{2}\log(2\pi)
        -\frac{1}{2}\log|\bV|
        -\frac{n}{2}\log\lambda
        - \frac{1}{2\lambda}
            (\by-\bX\bbeta)\T\bV\inv(\by-\bX\bbeta)
\end{align}

And there is a known, closed-form, maximum likelihood estimator for its two parameters:
\begin{align}
	\widehat{\bbeta}    &= (\bX \T \bV \inv \bX) \inv \bX \T \bV \inv \by\\
    \widehat{\lambda}   &= \norm{(\bX\widehat{\bbeta} - \by)\T(\bX\widehat{\bbeta} - \by)}{2}
\end{align}

In the context of a genome scan, we want to caculate these maximum likelihood estimators for a single phenotype, $\by$, genomic similarity, $\bK$ residual variance, $\bD$, and many different values of $\bX$ --- recall that the genetic variant(s) being investigated are in $\bG$.
Also recall that, though we have described a parameterization that recapitulates a standard linear regression problem when $h^2$ is known, we do not, in general know the true value of $h^2$, so we will additionally want to optimize over all possible values of $h^2 \in [0, 1]$.

We now describe a matrix algebra trick that increases the up-front computational cost of fitting the linear regression model \cref{eq:gls}, but drastically decreases the computatational cost for each new value of $\bX$.


\section{Rotation to Independence}

Imagine we knew of a multiplier matrix, $\bM$ such that
\begin{align}
	\bM \T \bM = \bV \inv
\end{align}
 
Let us now consider a new, rotated phenotype vector, $\by_r = \bM \by$, a new covariate matrix, $\bX_r = \bM \bX$, and a new SLM.
\begin{align}
    \by_r &\sim \N(\bX_r \bbeta_r, \, \bI \lambda_r)
\end{align}

The log likelihood of this SLM is:
\begin{align}
    \ell(\bbeta_r, \lambda_r; \bX_r, \by_r) 
    &=    -\frac{n}{2}\log(2\pi)
          -\frac{1}{2}\log|\bI|
          -\frac{n}{2}\log\lambda_r
          -\frac{1}{2\lambda_r}
          (\by_r - \bX_r)\T (\by_r - \bX_r\bbeta_r)\\
    & =
          -\frac{n}{2}\log(2\pi)
          -\frac{n}{2}\log\lambda_r
          -\frac{1}{2\lambda_r}
          (\bM\by - \bM\bX\bbeta)\T (\bM\by - \bM\bX\bbeta)\\
    &=    -\frac{n}{2}\log(2\pi)
          -\frac{n}{2}\log\lambda_r
          -\frac{1}{2\lambda_r}
          (\by - \bX\bbeta)\T \bM\T \bM (\by - \bX\bbeta)\\
    &=    -\frac{n}{2}\log(2\pi)
          -\frac{n}{2}\log\lambda_r
          -\frac{1}{2\lambda_r}
          (\by-\bX\bbeta)\T \bV\inv (\by-\bX\bbeta)
\end{align}

and its ML estimators are:
\begin{align}
    \widehat{\bbeta_r}    &= (\bX_r \T \bX_r) \inv \bX_r \T \by_r\\
    \widehat{\lambda_r}   &= \norm{(\bX_r \widehat{\bbeta_r} - \by_r)\T(\bX_r \widehat{\bbeta_r} - \by_r)}{2}
\end{align}

% Recalling that We are now considering a model of the form:
% \begin{align}
%     \mathbf{My} &\sim \Norm(\mathbf{MX}\boldsymbol{\beta}_z, \, \mathbf{I}\sigma^2_\text{z})
% \end{align}


% The expectation and variance of $\by_m$ are:
% \begin{align}
% \E(\by_m)	&= \E(\bM \by) \\
% 			&= \bM \E(\by)\\
% \V(\by_m)	&= \V(\bM \by)\\
% 			&= \bM \V(\by) \bM\T
% \end{align}


% Note the similarity of their log likelihoods:

Thus we have established that, given $\bM$ we can fit the model


\section{Homoscedastic Model}

[D = I]
Kang proposed the following [cite EMMA].




\section{Maximum Likelihood Estimation for the Heteroscedastic Model}

The above $\bM$ relied on [point out where D=I is required] [in step X].
Generally, however, some phenotypes are known with more certainty than others.
Want to have a diagonal D, but not necessarily I.

\subsection{Proposal}

\begin{align}
	\bM &= (\bLL + \delta \bI)^{-\frac{1}{2}}\bUL\T \bD^{-\frac{1}{2}}\\
\intertext{where}
	\bL &= \bD^{-\frac{1}{2}}\bK \bD^{-\frac{1}{2}}\\
\intertext{and}
	\bL &= \bUL \bLL \bUL^T
\end{align}
is its eigen decomposition


\subsection{Validity}

To be a valid multiplier matrix, $\bM$ must have the property:
\[
	\bM \T \bM = \bV\inv
\]

% I verify this by showing that
% \[
% 	\bV \bM\T \bM = \bM\T \bM \bV = \bI
% \]

% I verify by direct calculation:



\begin{proof}
First, derive a useful form of $\bV$.
\begin{align}
\bV &= \bK + \delta \bD   																								\tag*{definition}\\
    &= \bD\halfpow \bD\neghalfpow (\bK + \delta \bD)                                                                 	\tag*{pre-multiply by $\bD\halfpow \bD\neghalfpow = \bI$}\\
    &= \bD\halfpow \bD\neghalfpow (\bK + \delta \bD) \bD\neghalfpow \bD\halfpow 										\tag*{post-multiply by $\bD\neghalfpow \bD\halfpow = \bI$}\\
    &= \bD\halfpow(\bD\neghalfpow \bK \bD\neghalfpow + \delta \bD\neghalfpow \bD \bD\neghalfpow)\bD\halfpow 			\tag*{distribute $\bD\neghalfpow$ in}\\
    &= \bD\halfpow(\bD\neghalfpow \bK \bD\neghalfpow + \delta \bI) \bD\halfpow                                  		\tag*{definition of root inverse}\\
    &= \bD\halfpow(\bL + \delta \bI) \bD\halfpow                                                                 		\tag*{define: $\bL = \bD\neghalfpow\bK \bD\neghalfpow$}\\
    &= \bD\halfpow(\bUL \bLL \bUL \T + \delta \bI) \bD\halfpow                                                			\tag*{eigen decomposition of $\bL$}\\
    &= \bD\halfpow(\bUL \bLL \bUL \T + \delta \bUL \bUL \T)\bD\halfpow                                         			\tag*{property of eigen vectors}\\
    &= \bD\halfpow\bUL (\bLL + \delta \bI) \bUL \T \bD\halfpow                                                			\tag*{distributive property}
\end{align}
and invert it
\begin{align}
\bV\inv &= \left( \bD\halfpow\bUL (\bLL + \delta \bI) \bUL \T \bD\halfpow \right)\inv  											\tag*{definition}\\
    	&= \left( \bD\halfpow \right)\inv \left( \bUL (\bLL + \delta \bI) \bUL\T \right)\inv \left( \bD\halfpow \right)\inv     \tag*{inverse of product}\\
    	&= \bD\neghalfpow \left( \bUL (\bLL + \delta \bI) \bUL\T \right)\inv \bD\neghalfpow    									\tag*{inverse of diagonal matrix}\\
    	&= \bD\neghalfpow \bUL (\bLL + \delta \bI)\inv \bUL\T  \bD\neghalfpow               									\tag*{inverse of eigen decomposition}
\end{align}
and compare to $\bM\T \bM$
\begin{align}
\bM \T \bM 	&= \left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow 	\right)\T   \left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow	\right) 	\tag*{definition}\\
			&= \left(\bD\neghalfpow \bUL (\bLL + \delta \bI)\neghalfpow 	\right)  \left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow		\right)		\tag*{transpose of product}\\
			&= \bD\neghalfpow \bUL \left( (\bLL + \delta \bI)\neghalfpow 	(\bLL + \delta \bI)\neghalfpow \right)	\bUL\T \bD\neghalfpow						\tag*{associative property}\\
			&= \bD\neghalfpow \bUL (\bLL + \delta \bI)\inv \bUL\T \bD\neghalfpow																				\tag*{definition of root inverse}
			% &= \bD\neghalfpow \left( \bUL (\bLL + \delta \bI) \bUL\T \right)\inv \bD\neghalfpow 																\tag*{inverse of eigen decomposition}\\
\end{align}
\end{proof}

% Now we have
% \begin{align}
% \bM\T \bM 	&= 	\left( \bD\halfpow\bUL (\bLL + \delta \bI) \bUL \T \bD\halfpow 		\right)		
% 					\left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow 	\right)\T
% 					\left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow 	\right) 				\tag*{definitions}\\
% 				&= 	\left( \bD\halfpow\bUL (\bLL + \delta \bI) \bUL\T \bD\halfpow 		\right)
% 					\left(\bD\neghalfpow \bUL (\bLL + \delta \bI)\neghalfpow 	\right)
% 					\left((\bLL + \delta \bI)\neghalfpow \bUL\T \bD\neghalfpow 	\right)					\tag*{transpose of product}\\
% 				&= 	\bD\halfpow\bUL (\bLL + \delta \bI) \bUL\T \left(\bD\halfpow 		
% 					\bD\neghalfpow \right) \bUL \left((\bLL + \delta \bI)\neghalfpow
% 					(\bLL + \delta \bI)\neghalfpow \right) \bUL\T \bD\neghalfpow						\tag*{associative property of matrix multiplication}\\
% 				&= 	\bD\halfpow\bUL (\bLL + \delta \bI) \bUL\T
% 					 \bUL \left(\bLL + \delta \bI \right)\inv  \bUL\T \bD\neghalfpow					\tag*{definition of root and root inverse}\\
% 				&= 	\bD\halfpow\bUL (\bLL + \delta \bI) \left(\bLL + \delta \bI \right)\inv  \bUL\T \bD\neghalfpow					\tag*{property of eigenvector matrix}\\
% 				&= 	\bD\halfpow\bUL  \bUL\T \bD\neghalfpow					\tag*{definition of inverse}\\
% 				&= 	\bD\halfpow \bD\neghalfpow					\tag*{property of eigenvector matrix}\\
% 				&= \bI 													\tag*{definition of inverse}
% \end{align}

% 	&= [D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) U_L^T D^{\frac{1}{2}}] [D^{-\frac{1}{2}}U_L(\Lambda_L+\delta I)^{-\frac{1}{2}}][(D^{-\frac{1}{2}}U_L(\Lambda_L+\delta I)^{-\frac{1}{2}})^T]\\
%     		&= D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) U_L^T D^{\frac{1}{2}}D^{-\frac{1}{2}}U_L(\Lambda_L+\delta I)^{-\frac{1}{2}}(\Lambda_L+\delta I)^{-\frac{1}{2}T} U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) U_L^T D^{\frac{1}{2}}D^{-\frac{1}{2}}U_L(\Lambda_L+\delta I)^{-1} U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) U_L^T U_L(\Lambda_L+\delta I)^{-1} U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}}U_L (\Lambda_L + \delta I) (\Lambda_L+\delta I)^{-1} U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}}U_L U_L^T D^{-\frac{1}{2}T} \\
%     		&= D^{\frac{1}{2}} D^{-\frac{1}{2}T} \\
%     		&= I
% \end{align}


\section{Simulation Results}

\section{Software}